# Networked Programs
* Quick introduction to how the network works

## Transport Control Protocol (TCP)
* Built on top of IP (Internet Protocol)
* Assumes IP might lose some data - stores and retransmits data if it seems to be lost
* Handles "flow control" using a transmit window
* Provides a nice reliable pipe

## TCP Connections / Sockets

"In computer networking, an internet socket or network socket is an endpoint of a bidirectional inter-process comunication flow across an Internet Protocol-based computer network, such as the internet"

## TCP Port Numbers
* A port is an application-specific or process-specific software communications endpoint
* It allows multiple networked applications to coexist on the same server
* There is a list of well-known TCP port numbers

## Common TCP Ports
* Telnet (23) - Login
* SSH (22) - Secure Login
* HTTP (80)
* etc

## Sockets in Python
* Python has built-in support for TCP Sockets

```Python
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('data.pr4e.org'), 80) # host, port
```

## Application protocols
* Since TCP (and Python) gives us a reliable socket, what do we want to do with the socket? What problem do we want to solve?
* Application protocols
    * Mail
    * World wide web

## HTTP - Hypertext Transfer Protocol
* The dominant Application Layer Protocol on the Internet
* Invented for the web - to retrieve HTML, Images, Documents, etc
* Extended to be data in addition to documents - RSS, Web Services, etc..
* Basic Concept
    - Make a connection
    - Request a document
    - Retrieve a document
    - Close the connection

## HTTP
* The HyperText Transfer Protocol is the set of rules to allow browsers to retrieve web documents from servers over the internet

## What is a protocol?
* A set of rules that all parties follow so we can predict each other's behaviour
* And not bump into each other
    * On two-way roads in USA, drive on the right-hand side of the road
    * On two-way roads in the UK, drive on the left-hand side of the road

## Getting data from the server
* Each user the clicks on an anchor tag with an href= value to switch to a new page, the browser makes a connection to the web server and issues a "GET" request - to GET the content of the page at the specified URL
* The server returns the HTML document to the browser which formats and displays the document to the user.

## Internet standards
* The standards for all the internet protocols (inner workings) are developed by an organisation
* Internet engineering task force
* www.ietf.org
* Standards are called "RFCs" - "Request for Comments"

## Accurate hacking in the movies
* Matrix Reloaded
* Bourne Ultimatum
* Die Hard 4
* ...

## An HTTP Request in Python
```Python
import socket

mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('data.pr4e.org', 80))
cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\n\n'.encode()
mysock.send(cmd)

while True:
    mysock.recv(512)
    if len(data) < 1:
        break:
    print(data.decode())
mysock.close()
```

Returns headers/metadata and a break between the actual text or content with the headers.

## About characters and strings
* ASCII

## ASCII
* Standardised character set for latin character

## Representing simple strings
* Each character is represented by a number between 0 and 256 stored in 8 bits of memory
* We refer to 8 bits of memory as a "byte" of memory - (i.e. my disk drive contains 3 Terabytes of memory)
* The `ord()` function tells us the numeric value of a simple ASCII character

## Unicode
* Computers can store and understand a much larger character set now

## Multi-Byte Characters
* To represent the wide range of characters computers must handle we represent characters with more than one byte
    * UTF-16 - Fixed length - Two bytes
    * UTF-32 - Fixed length - Four bytes
    * UTF-8 - 1-4 bytes
        * Upwards compatible with ASCII
        * Automatic detection between ASCII and UTF-8
        * UTF-8 is recommended practice for encoding data to be exchanged between systems

## Two kinds of strings in Python
* In Python 3, all strings are Unicode

## Python3 and Unicode
* In Python 3, all strings internally are UNICODE
* Working with string variables in Python programs and reading data from files usually "just works"
* When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8)

## Python Strings to Bytes
* When we talk to an external resource like a network socket we sends bytes, so we need to encode Python 3 strings into a given character encoding
* When we read data from an external resource, we must decode it based on the character set so it is properly represented in Python 3 as a string
* Encode when send, decode when receiving from network

## Using urllib in Python
* Since HTTP is so common, we have a library that does all the socket work for us and makes web pages look like a file

```Python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')
for line in fhand:
    print(line.decode().strip())
```

* Contains no headers! Use a different method in the library to retrieve the headers

## Like a file...

```Python
import urllib.request, urllib.parse, urllib.error

counts = dict()
fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')
for line in fhand:
    for word in words:
        counts[word] = counts.get(word, 0) + 1
print(counts)
```

## Reading web pages
* Will have html tags when retrieved as a text file

## What is web scraping?
* When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages
* Search engines scrape web pages - we call this "spidering the web" or "web crawling"

## Why scrape?
* Pull data - particularly social data - who links to who?
* Get your own data back out of some system that has no "export capability"
* Monitor a site for new information
* Spider the web to make a database for a search engine

## Scraping web pages
* There is some controversy about web page scraping and some sites are a bit snippy about it
* Republishing copyrighted information is not allowed
* Violating terms of service is not allowed

## The easy way - Beautiful Soup
* You could do string searches the hard way
* Or use the free software library called Beautiful Soup

## Summary
* Sockets
* Application protocols 
* HTTP is a simple yet powerful protocol
* Python has good support for sockets, HTTP and HTML parsing
